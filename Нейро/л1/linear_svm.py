import numpy as np
from random import shuffle

def svm_loss_naive(W, X, y, reg):
  """
  SVM функция потерь, наивная реализация (с циклами).

  D - длина вектора данных, C - число классов,
  операции выполняются над миниблоками с N примерами.

  Входы:
  - W: numpy масссив формы (D, C), содержащий веса.
  - X: numpy масссив формы (N, D), содержащий миниблок данных.
  - y: numpy масссив формы (N,), содержащий обучающие метки; y[i] = c означает,
    что X[i] имеет метку c, где 0 <= c < C.
  - reg: (float) коэффициент регуляризации

  Возращает кортеж из:
  - потери в формате single float
  - градиент по отношению к весам W; массив такой же формы как W
  """
    
  dW = np.zeros(W.shape) # инициализируем градиент
  num_classes = W.shape[1]
  num_train = X.shape[0]  
  
  
  #############################################################################
  # ЗАДАНИЕ:                                                                  #
  # Вычислите градиент функции потерь по отношению к W и сохраните его в dW.  #
  # Вместо того, чтобы сначала вычислять функцию потерь, а затем вычислять    #
  # производную, вычисляйте производную в процессе вычисления функции потерь. #
  # Поэтому Вам просто модифицируйте код приведенный выше, включив   него     #
  # вычисление требуемого градиента                                           #
  #############################################################################
  
  # вычисление функции потерь по всем обучающим примерам X[i]   
  loss = 0.0
  for i in range(num_train):
    scores = X[i].dot(W)
    correct_class_score = scores[y[i]]
    
    # вычисление функции потерь L_i для X[i] и её аналитического градиента dwL_i 
    count=0 #число классов, не удовлетворяющих отступу
    for j in range(num_classes):
        #вычисляем отступ для класса j
        margin = scores[j] - correct_class_score + 1 # note delta = 1 
        # если отступ >0 и j!= корректному классу y[i], 
        # то увеличиваем count - счетчик не корр. классов и вычисл. вектор градиента для некорр. классa
        if margin>0:
            if j != y[i]:
                count+=1
                dW[:,j]+=X[i,:].T #суммируем все градиенты по миниблоку
        # если класс корректн., то для него Li=0
        if j == y[i]:
              margin=0
        if margin > 0:
            loss += margin
    # вычисляем вектор градиента для коррект. класса
    dW[:,y[i]]+=(-count)*(X[i,:].T)   #суммируем все градиенты по миниблоку   
            
  # До этого момента потери - это сумма потерь по всем обучающим примерам, 
  # но мы хотим получить средние потери поэтому делим на num_train.
  loss /= num_train
  dW /=num_train
  # добавляем регуляризацию
  loss += reg * np.sum(W * W)
  dW+=reg*2*W
  
  return loss, dW


def svm_loss_vectorized(W, X, y, reg):
  """
  SVM функцмя потерь, векторизованная реализация.

  Входы и выходы такие же, как и у svm_loss_naive.
  """
  loss = 0.0
  dW = np.zeros(W.shape) # инициализируем градиент нулями

  #############################################################################
  # ЗАДАНИЕ:                                                                  #
  # Реализуйте векторизованную версию кода для вычисления SVM функции потерь. #
  # Сохраните результат в переменной loss.                                    #
  #############################################################################
  N = len(y)
  scores = X.dot(W) # N х C
  correct = scores[range(N),y].reshape((N,1)) # рейтинг корр.класса
  margin = np.maximum(scores + 1.0 - correct, 0.0) # потери по всем примерам
  margin[range(N), y] = 0.0 # если класс корректн., то для него Li=0 
  loss = margin.sum() / N # средние потери
  loss += 0.5 * reg * np.sum(W * W) # потери с учетом регуляризации
  pass
  #############################################################################
  #                             КОНЕЦ ВАШЕГО КОДА                             #
  #############################################################################


  #############################################################################
  # ЗАДАНИЕ:                                                                  #  
  # Реализуйте векторизованную версию кода для вычисления градиента SVM       #
  # функции потерь. Сохраните результат в переменной dW.                      #
  # Совет: Используйте некоторые промежуточные значения, которые были         #
  # получены при вычислении функции потерь.                                   #
  #############################################################################
  # вычисляем градиент SVM функции потерь по W
  # # создаем бинарную матрицу c размером, равным размеру margin
  # в позициях матрицы, где условие SVM не выполняется, будут 1.0 , иначе 0.0 
  dscores = (margin > 0).astype("float32") # N х C
  # в каждую строку dscores в позицию корректного класса вносим суммарное число
  # рейтингов классов, нарушающих условие SVM –sum(1 (margin>0))
  dscores[range(len(y)), y] = -dscores.sum(axis=1)
  # предварительно делим на N для вычисления среднего
  dscores /= N
  # градиент регуляризационного члена
  dreg = reg * W
  # полный градиент функции потерь
  dW = (X.T.dot(dscores) + dreg) #(D,N)*(N,C)-> (D,C)
  pass
  #############################################################################
  #                             КОНЕЦ ВАШЕГО КОДА                             #
  #############################################################################
  return loss, dW